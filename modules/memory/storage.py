"""
å­˜å‚¨å±‚ - ChromaDB äº¤äº’ã€å¼‚æ­¥å­˜å‚¨
"""
import os
import time
import uuid
import json
import queue
import threading
import chromadb
from concurrent.futures import ThreadPoolExecutor

from ..config import data_dir
from .config import STRONG_SIMILARITY_THRESHOLD, PREFERENCE_PATTERNS
from .logger import get_logger, get_log_path
from .analyzers import TextAnalyzer
from .conflict import ConflictResolver, ConflictDetector, extract_user_input

logger = get_logger()


class MemoryStorage:
    """è®°å¿†å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self):
        self.enabled = False
        self.long_term = None
        self.emotional = None
        self.working = None
        self._collections = []
        self._conflict_resolver = None
        
        # å¼‚æ­¥å­˜å‚¨é˜Ÿåˆ—
        self._store_queue = queue.Queue()
        self._update_queue = queue.Queue()
        
        # çº¿ç¨‹æ± ï¼ˆå¢åŠ workeræ•°é‡ä»¥æ”¯æŒå¹¶è¡ŒæŸ¥è¯¢ï¼‰
        self._executor = ThreadPoolExecutor(max_workers=10)
        
        self._initialize_storage()
    
    def _initialize_storage(self):
        """åˆå§‹åŒ– ChromaDB å­˜å‚¨"""
        os.makedirs(data_dir, exist_ok=True)
        
        logger.info("=" * 50)
        logger.info("äººç±»åŒ–è®°å¿†ç³»ç»Ÿ æ­£åœ¨åˆå§‹åŒ–ï¼ˆä½å»¶è¿Ÿæ¨¡å¼ï¼‰")
        
        try:
            self.client = chromadb.PersistentClient(path=data_dir)
            
            self.long_term = self.client.get_or_create_collection(
                name="long_term_memory",
                metadata={"description": "å·©å›ºåçš„é•¿æœŸè®°å¿†"}
            )
            self.emotional = self.client.get_or_create_collection(
                name="emotional_memory",
                metadata={"description": "å¸¦æœ‰å¼ºçƒˆæƒ…æ„Ÿçš„è®°å¿†"}
            )
            self.working = self.client.get_or_create_collection(
                name="working_memory",
                metadata={"description": "å¾…å·©å›ºçš„å·¥ä½œè®°å¿†"}
            )
            
            self._collections = [
                (self.emotional, "æƒ…æ„Ÿè®°å¿†"),
                (self.long_term, "é•¿æœŸè®°å¿†"),
                (self.working, "å·¥ä½œè®°å¿†")
            ]
            
            self._conflict_resolver = ConflictResolver(self._collections)
            self.enabled = True
            
            logger.info(f"å­˜å‚¨è·¯å¾„: {data_dir}")
            logger.info(f"é•¿æœŸè®°å¿†: {self.long_term.count()} | æƒ…æ„Ÿè®°å¿†: {self.emotional.count()} | å·¥ä½œè®°å¿†: {self.working.count()}")
            logger.info(f"æ—¥å¿—æ–‡ä»¶: {get_log_path()}")
            logger.info("è®°å¿†ç³»ç»Ÿå·²å°±ç»ª")
            
            self._start_background_workers()
            
        except Exception as e:
            logger.error(f"è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.enabled = False
        
        logger.info("=" * 50)
    
    def _start_background_workers(self):
        """å¯åŠ¨åå°å·¥ä½œçº¿ç¨‹"""
        self._store_thread = threading.Thread(target=self._store_worker, daemon=True)
        self._store_thread.start()
        
        self._update_thread = threading.Thread(target=self._update_worker, daemon=True)
        self._update_thread.start()
    
    def _store_worker(self):
        """åå°å­˜å‚¨çº¿ç¨‹"""
        while True:
            try:
                task = self._store_queue.get()
                if task is None:
                    logger.debug("å­˜å‚¨çº¿ç¨‹æ”¶åˆ°é€€å‡ºä¿¡å·")
                    break
                self._do_store_memory(*task)
                self._store_queue.task_done()
            except Exception as e:
                logger.error(f"å­˜å‚¨çº¿ç¨‹å¼‚å¸¸: {e}")
    
    def _update_worker(self):
        """æ‰¹é‡æ›´æ–°çº¿ç¨‹"""
        pending_updates = []
        last_flush = time.time()
        
        while True:
            try:
                try:
                    task = self._update_queue.get(timeout=1.0)
                    if task is None:
                        break
                    pending_updates.append(task)
                    self._update_queue.task_done()
                except queue.Empty:
                    pass
                
                if pending_updates and (len(pending_updates) >= 10 or time.time() - last_flush > 1.0):
                    self._flush_updates(pending_updates)
                    pending_updates = []
                    last_flush = time.time()
                    
            except Exception:
                pass
    
    def _flush_updates(self, updates):
        """æ‰¹é‡æ‰§è¡Œæ›´æ–°"""
        for memory_id, collection in updates:
            try:
                result = collection.get(ids=[memory_id], include=["metadatas"])
                if result['metadatas']:
                    meta = result['metadatas'][0]
                    meta['access_count'] = meta.get('access_count', 0) + 1
                    meta['last_access'] = time.time()
                    collection.update(ids=[memory_id], metadatas=[meta])
            except Exception:
                pass
    
    def _is_review_question(self, text: str) -> bool:
        """
        æ£€æµ‹æ˜¯å¦ä¸ºå›é¡¾æ€§æé—®ï¼Œå¦‚â€œä½ è¿˜è®°å¾—æˆ‘å–œæ¬¢â€¦å—â€â€œæˆ‘ä¹‹å‰è¯´è¿‡â€¦â€ç­‰
        """
        review_patterns = [
            'ä½ è¿˜è®°å¾—', 'è¿˜è®°å¾—æˆ‘', 'æˆ‘ä¹‹å‰è¯´è¿‡', 'æˆ‘ä»¥å‰è¯´è¿‡', 'æˆ‘åˆšæ‰è¯´', 'æˆ‘åˆšåˆšè¯´',
            'æˆ‘æ›¾ç»è¯´', 'æˆ‘åˆšæ‰æåˆ°', 'æˆ‘åˆšåˆšæåˆ°', 'æˆ‘ä¹‹å‰æåˆ°', 'æˆ‘ä»¥å‰æåˆ°',
            'ä½ è®°å¾—', 'è®°å¾—æˆ‘', 'ä½ èƒ½å›å¿†', 'ä½ èƒ½æƒ³èµ·', 'ä½ èƒ½è®°å¾—', 'ä½ èƒ½å‘Šè¯‰æˆ‘æˆ‘',
            'æˆ‘é—®è¿‡', 'æˆ‘è¯´è¿‡', 'æˆ‘æè¿‡', 'æˆ‘æåˆ°è¿‡', 'æˆ‘è®²è¿‡', 'æˆ‘è®²åˆ°è¿‡',
            'ä½ çŸ¥é“æˆ‘', 'ä½ çŸ¥é“æˆ‘å–œæ¬¢', 'ä½ çŸ¥é“æˆ‘è®¨åŒ', 'ä½ çŸ¥é“æˆ‘æœ€å–œæ¬¢', 'ä½ çŸ¥é“æˆ‘æœ€è®¨åŒ',
            'ä½ èƒ½çŒœ', 'ä½ çŒœæˆ‘', 'ä½ èƒ½æƒ³åˆ°', 'ä½ èƒ½æƒ³åˆ°æˆ‘', 'ä½ èƒ½æƒ³åˆ°æˆ‘å–œæ¬¢', 'ä½ èƒ½æƒ³åˆ°æˆ‘è®¨åŒ',
            'ä½ èƒ½æƒ³åˆ°æˆ‘æœ€å–œæ¬¢', 'ä½ èƒ½æƒ³åˆ°æˆ‘æœ€è®¨åŒ',
        ]
        # ç–‘é—®å¥æ ‡è®°
        question_marks = ['å—', '?', 'ï¼Ÿ']
        if any(p in text for p in review_patterns) and any(q in text for q in question_marks):
            return True
        # ä¹Ÿå…è®¸â€œä½ è¿˜è®°å¾—æˆ‘å–œæ¬¢åƒä»€ä¹ˆâ€è¿™ç±»æ— é—®å·ä½†æ˜æ˜¾å›é¡¾æ€§æé—®
        if any(p in text for p in review_patterns):
            return True
        return False

    def store_memory(self, conversation: str, current_emotion: str) -> str:
        """å¼‚æ­¥å­˜å‚¨è®°å¿†ï¼ˆéé˜»å¡ï¼‰"""
        if not self.enabled:
            return current_emotion

        clean_conv = TextAnalyzer.clean_text(conversation)
        if len(clean_conv) < 5:
            return current_emotion

        # å›é¡¾æ€§æé—®ä¸å­˜å‚¨ä¸ºè®°å¿†
        if self._is_review_question(clean_conv):
            logger.debug(f"[è¿‡æ»¤] å›é¡¾æ€§æé—®æœªå­˜å‚¨: {clean_conv}")
            return current_emotion

        entities = TextAnalyzer.extract_entities(clean_conv)
        emotion_type, emotion_intensity = TextAnalyzer.analyze_emotion(clean_conv)
        importance = TextAnalyzer.calculate_importance(clean_conv, entities, emotion_type, emotion_intensity)

        new_emotion = emotion_type if emotion_type != 'neutral' else current_emotion

        self._store_queue.put((clean_conv, entities, emotion_type, emotion_intensity, importance))
        return new_emotion
    
    def _do_store_memory(self, clean_conv, entities, emotion_type, emotion_intensity, importance):
        """å®é™…å­˜å‚¨æ“ä½œï¼ˆåå°çº¿ç¨‹ï¼‰"""
        memory_id = str(uuid.uuid4())
        user_input = extract_user_input(clean_conv)
        has_preference = ConflictDetector.detect_preference_conflict(user_input)
        preference_category = ConflictDetector.get_preference_category(user_input) if has_preference else None
        preference_polarity = None
        if has_preference:
            if any(p in user_input for p in PREFERENCE_PATTERNS['negative']):
                preference_polarity = 'negative'
            elif any(p in user_input for p in PREFERENCE_PATTERNS['positive']):
                preference_polarity = 'positive'

        metadata = {
            "timestamp": time.time(),
            "access_count": 0,
            "last_access": time.time(),
            "importance": importance,
            "emotion_type": emotion_type,
            "emotion_intensity": emotion_intensity,
            "entities": json.dumps(list(entities.keys())) if entities else "[]",
            "consolidated": False,
            "preference": bool(has_preference),
            "preference_category": preference_category or "",
            "preference_polarity": preference_polarity or "",
            "preference_entities": json.dumps(list(TextAnalyzer.extract_noun_entities(user_input))) if has_preference else "[]"
        }
        
        try:
            # æ™ºèƒ½å†²çªæ£€æµ‹ä¸è¦†ç›–
            if self._conflict_resolver:
                self._conflict_resolver.smart_conflict_override(clean_conv, entities)
            
            # æ ¹æ®é‡è¦æ€§å’Œæƒ…æ„Ÿå­˜å‚¨åˆ°ä¸åŒé›†åˆ
            if emotion_intensity >= 2 or emotion_type == 'important':
                self.emotional.add(documents=[clean_conv], metadatas=[metadata], ids=[memory_id])
                logger.info(f"[å­˜å‚¨] æƒ…æ„Ÿè®°å¿† | {clean_conv[:50]}... | æƒ…æ„Ÿ={emotion_type} å¼ºåº¦={emotion_intensity}")
            elif importance >= 0.35:
                metadata['consolidated'] = True
                self.long_term.add(documents=[clean_conv], metadatas=[metadata], ids=[memory_id])
                logger.info(f"[å­˜å‚¨] é•¿æœŸè®°å¿† | {clean_conv[:50]}... | é‡è¦åº¦={importance:.2f}")
            else:
                self.long_term.add(documents=[clean_conv], metadatas=[metadata], ids=[memory_id])
                logger.debug(f"[å­˜å‚¨] å·¥ä½œè®°å¿† | {clean_conv[:50]}... | é‡è¦åº¦={importance:.2f}")
                
        except Exception as e:
            logger.error(f"[å­˜å‚¨å¤±è´¥] {clean_conv[:30]}... | é”™è¯¯: {e}")
    
    def get_collections(self):
        """è·å–æ‰€æœ‰é›†åˆ"""
        return self._collections
    
    def get_executor(self):
        """è·å–çº¿ç¨‹æ± """
        return self._executor
    
    def get_update_queue(self):
        """è·å–æ›´æ–°é˜Ÿåˆ—"""
        return self._update_queue
    
    def cleanup_old_memories(self):
        """æ¸…ç†æ—§è®°å¿†"""
        if not self.enabled:
            return
        
        total_deleted = 0
        for collection, name in [(self.working, "å·¥ä½œè®°å¿†"), (self.long_term, "é•¿æœŸè®°å¿†")]:
            try:
                results = collection.get(include=["metadatas"])
                to_delete = [
                    doc_id for doc_id, meta in zip(results.get('ids', []), results.get('metadatas', []))
                    if meta and TextAnalyzer.calculate_memory_strength(meta) < 0.1 and meta.get('importance', 0.5) < 0.5
                ]
                if to_delete:
                    collection.delete(ids=to_delete)
                    total_deleted += len(to_delete)
                    logger.info(f"[æ¸…ç†] [{name}] åˆ é™¤ {len(to_delete)} æ¡ä½å¼ºåº¦è®°å¿†")
            except Exception as e:
                logger.error(f"[æ¸…ç†å¤±è´¥] [{name}] {e}")
        
        if total_deleted > 0:
            logger.info(f"[æ¸…ç†å®Œæˆ] å…±åˆ é™¤ {total_deleted} æ¡è®°å¿†")

        # å…¨é‡è¯­ä¹‰çŸ›ç›¾æ£€æµ‹ä¸è¦†ç›–
        self.resolve_all_contradictions()
    
    def get_stats(self):
        """è·å–å­˜å‚¨ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'working_memory': self.working.count() if self.working else 0,
            'long_term': self.long_term.count() if self.long_term else 0,
            'emotional': self.emotional.count() if self.emotional else 0,
            'pending_stores': self._store_queue.qsize(),
            'pending_updates': self._update_queue.qsize(),
        }

    def resolve_all_contradictions(self):
        """å¯¹æ‰€æœ‰è®°å¿†è¿›è¡Œå¥æ„ç†è§£å¹¶æ¸…ç†çŸ›ç›¾å¯¹"""
        if not self.enabled or not self._conflict_resolver:
            return
        self._conflict_resolver.resolve_all_semantic_conflicts()
    
    def force_update_memory(self, old_info: str, new_info: str) -> bool:
        """å¼ºåˆ¶æ›´æ–°è®°å¿†"""
        if not self.enabled:
            return False
        
        logger.info(f"[å¼ºåˆ¶æ›´æ–°] æ—§: {old_info} -> æ–°: {new_info}")
        
        deleted_count = 0
        for collection, layer_name in self._collections:
            try:
                results = collection.query(
                    query_texts=[old_info],
                    n_results=10,
                    include=["documents", "distances"]
                )
                docs = results.get('documents', [[]])[0]
                distances = results.get('distances', [[]])[0]
                ids = results.get('ids', [[]])[0]
                
                to_delete = [
                    doc_id for doc, dist, doc_id in zip(docs, distances, ids)
                    if dist < 0.8
                ]
                if to_delete:
                    collection.delete(ids=to_delete)
                    deleted_count += len(to_delete)
                    logger.info(f"[å¼ºåˆ¶æ›´æ–°] ä»[{layer_name}]åˆ é™¤ {len(to_delete)} æ¡")
                    print(f"   â”œâ”€ ä»[{layer_name}]åˆ é™¤ {len(to_delete)} æ¡")
            except Exception as e:
                logger.error(f"[å¼ºåˆ¶æ›´æ–°å¤±è´¥] [{layer_name}] {e}")
        
        # å­˜å‚¨æ–°ä¿¡æ¯
        self.store_memory(f"ç”¨æˆ·æ›´æ­£: {new_info}", 'neutral')
        logger.info(f"[å¼ºåˆ¶æ›´æ–°] æ–°è®°å¿†å·²å­˜å‚¨")
        print(f"   â””â”€ æ–°è®°å¿†å·²å­˜å‚¨")
        
        return deleted_count > 0
    
    def clear_about(self, keyword: str) -> int:
        """æ¸…é™¤å…³äºæŸä¸ªå…³é”®è¯çš„æ‰€æœ‰è®°å¿†"""
        if not self.enabled:
            return 0
        
        logger.info(f"[æ¸…é™¤è®°å¿†] å…³é”®è¯: {keyword}")
        print(f"\n[ğŸ—‘ï¸ æ¸…é™¤è®°å¿†] å…³é”®è¯: {keyword}")
        
        deleted_count = 0
        for collection, layer_name in self._collections:
            try:
                results = collection.query(
                    query_texts=[keyword],
                    n_results=20,
                    include=["documents", "distances"]
                )
                ids = results.get('ids', [[]])[0]
                distances = results.get('distances', [[]])[0]
                
                to_delete = [doc_id for doc_id, dist in zip(ids, distances) if dist < 0.7]
                if to_delete:
                    collection.delete(ids=to_delete)
                    deleted_count += len(to_delete)
                    logger.info(f"[æ¸…é™¤è®°å¿†] ä»[{layer_name}]åˆ é™¤ {len(to_delete)} æ¡")
                    print(f"   â”œâ”€ ä»[{layer_name}]åˆ é™¤ {len(to_delete)} æ¡")
            except Exception as e:
                logger.error(f"[æ¸…é™¤å¤±è´¥] [{layer_name}] {e}")
        
        logger.info(f"[æ¸…é™¤è®°å¿†] å…±åˆ é™¤ {deleted_count} æ¡")
        print(f"   â””â”€ å…±åˆ é™¤ {deleted_count} æ¡è®°å¿†")
        return deleted_count
    
    def close(self):
        """å…³é—­å­˜å‚¨ç³»ç»Ÿ"""
        logger.info("[å…³é—­] æ­£åœ¨ä¿å­˜æœªå®Œæˆçš„è®°å¿†...")
        print(" [è®°å¿†ç³»ç»Ÿ] æ­£åœ¨ä¿å­˜æœªå®Œæˆçš„è®°å¿†...")
        
        self._store_queue.join()
        self._store_queue.put(None)
        self._update_queue.join()
        self._update_queue.put(None)
        self._executor.shutdown(wait=True)
        
        logger.info("[å…³é—­] æ‰€æœ‰è®°å¿†å·²ä¿å­˜å®Œæ¯•")
        logger.info(f"[å…³é—­] æ—¥å¿—æ–‡ä»¶ä½ç½®: {get_log_path()}")
        print(" [è®°å¿†ç³»ç»Ÿ] æ‰€æœ‰è®°å¿†å·²ä¿å­˜å®Œæ¯•")
        print(f" [è®°å¿†ç³»ç»Ÿ] æ—¥å¿—æ–‡ä»¶: {get_log_path()}")
